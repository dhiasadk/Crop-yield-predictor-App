{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pyspark in c:\\users\\rabhi\\appdata\\roaming\\python\\python311\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\rabhi\\appdata\\roaming\\python\\python311\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in c:\\users\\rabhi\\appdata\\roaming\\python\\python311\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize findspark and Spark session\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import required libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "# Data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning preprocessing from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler as SklearnStandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, to_date, year, month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001C8BD646050>\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Demeter\") \\\n",
    "    .getOrCreate()\n",
    "print(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset with semicolon separator\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the dataset\n",
    "    \n",
    "    Returns:\n",
    "        Spark DataFrame\n",
    "    \"\"\"\n",
    "    # Read CSV with semicolon separator\n",
    "    df = spark.read.option(\"sep\", \";\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(file_path)\n",
    "    \n",
    "    # Print original column names\n",
    "    print(\"Original Column Names:\", df.columns)\n",
    "    \n",
    "    # Rename columns to remove any unwanted characters\n",
    "    for old_col in df.columns:\n",
    "        new_col = old_col.strip()  # Remove leading/trailing whitespace\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "    # Print cleaned column names\n",
    "    print(\"Cleaned Column Names:\", df.columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data2(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data:\n",
    "    1. Handle missing values\n",
    "    2. Detect and handle outliers\n",
    "    3. Feature engineering\n",
    "    \n",
    "    Args:\n",
    "        df (Spark DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed Spark DataFrame\n",
    "    \"\"\"\n",
    "    # 3.1 Missing Value Handling\n",
    "    # Replace missing values with median for numeric columns\n",
    "    numeric_cols = ['Soil_pH', 'Temperature', 'Humidity', 'Wind_Speed', \n",
    "                    'N', 'P', 'K', 'Soil_Quality']\n",
    "    \n",
    "    # Function to calculate median safely\n",
    "    from pyspark.sql.functions import col, percentile_approx\n",
    "    \n",
    "    # Replace missing values\n",
    "    for col_name in numeric_cols:\n",
    "        # Calculate median using percentile_approx\n",
    "        median_val = df.select(percentile_approx(col(col_name), 0.5)).first()[0]\n",
    "        df = df.na.fill({col_name: median_val})\n",
    "    \n",
    "    # 3.2 Outlier Detection and Handling (Using IQR method)\n",
    "    def remove_outliers(dataframe, columns):\n",
    "        for col_name in columns:\n",
    "            # Calculate Q1, Q3, and IQR\n",
    "            q1 = dataframe.approxQuantile(col_name, [0.25], 0.01)[0]\n",
    "            q3 = dataframe.approxQuantile(col_name, [0.75], 0.01)[0]\n",
    "            iqr = q3 - q1\n",
    "            \n",
    "            # Define outlier bounds\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            \n",
    "            # Filter out outliers\n",
    "            dataframe = dataframe.filter(\n",
    "                (col(col_name) >= lower_bound) & \n",
    "                (col(col_name) <= upper_bound)\n",
    "            )\n",
    "        return dataframe\n",
    "    \n",
    "    df = remove_outliers(df, numeric_cols)\n",
    "    \n",
    "    # 3.3 Feature Engineering\n",
    "    # Parse Date column\n",
    "    df = df.withColumn(\"Date\", expr(\"to_date(Date, 'yyyy-MM-dd')\"))\n",
    "    \n",
    "    # Extract month and year from Date\n",
    "    df = df.withColumn(\"Month\", month(col(\"Date\")))\n",
    "    df = df.withColumn(\"Year\", year(col(\"Date\")))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data:\n",
    "    1. Handle missing values\n",
    "    2. Detect and handle outliers\n",
    "    3. Feature engineering\n",
    "    \n",
    "    Args:\n",
    "        df (Spark DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed Spark DataFrame\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import col, percentile_approx, lit, expr\n",
    "\n",
    "    # List of numeric columns\n",
    "    numeric_cols = ['Soil_pH', 'Temperature', 'Humidity', 'Wind_Speed', \n",
    "                    'N', 'P', 'K', 'Soil_Quality']\n",
    "\n",
    "    # Handle missing values for numeric columns by replacing with median\n",
    "    for col_name in numeric_cols:\n",
    "        median_val = df.select(percentile_approx(col(col_name), 0.5)).first()[0]\n",
    "        if median_val is not None:  # Fill only if median is computed\n",
    "            df = df.na.fill({col_name: median_val})\n",
    "\n",
    "    # Categorical columns (fill missing with a placeholder or default value)\n",
    "    df = df.na.fill({\"Crop_Type\": \"Unknown\", \"Soil_Type\": \"Unknown\"})\n",
    "\n",
    "    # Handle missing values for Date (fill with a default date if needed)\n",
    "    df = df.withColumn(\"Date\", expr(\"nvl(Date, '2000-01-01')\"))\n",
    "\n",
    "    # Remove outliers using IQR for numeric columns\n",
    "    def remove_outliers(dataframe, columns):\n",
    "        for col_name in columns:\n",
    "            q1, q3 = dataframe.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            dataframe = dataframe.filter((col(col_name) >= lower_bound) & \n",
    "                                         (col(col_name) <= upper_bound))\n",
    "        return dataframe\n",
    "\n",
    "    df = remove_outliers(df, numeric_cols)\n",
    "\n",
    "    # Feature Engineering: Convert Date and extract Month, Year\n",
    "    df = df.withColumn(\"Date\", expr(\"to_date(Date, 'yyyy-MM-dd')\"))\n",
    "    df = df.withColumn(\"Month\", month(col(\"Date\")))\n",
    "    df = df.withColumn(\"Year\", year(col(\"Date\")))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature Selection and Correlation Analysis\n",
    "def feature_correlation(df):\n",
    "    \"\"\"\n",
    "    Visualize feature correlations\n",
    "    \n",
    "    Args:\n",
    "        df (Spark DataFrame): Input DataFrame\n",
    "    \"\"\"\n",
    "    # Convert to Pandas for correlation visualization\n",
    "    pandas_df = df.toPandas()\n",
    "    \n",
    "    # Select numeric columns\n",
    "    numeric_cols = ['Soil_pH', 'Temperature', 'Humidity', 'Wind_Speed', \n",
    "                    'N', 'P', 'K', 'Soil_Quality', 'Crop_Yield']\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    correlation_matrix = pandas_df[numeric_cols].corr()\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_matrix.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def prepare_and_train_model(df):\n",
    "    \"\"\"\n",
    "    Prepare data for training and train the model.\n",
    "    \n",
    "    Args:\n",
    "        df (Spark DataFrame): Preprocessed DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains the trained model and test data\n",
    "    \"\"\"\n",
    "    # Validate data\n",
    "    required_columns = [\n",
    "        'Crop_Type', 'Soil_Type', 'Soil_pH', 'Temperature', \n",
    "        'Humidity', 'Wind_Speed', 'N', 'P', 'K', 'Crop_Yield'\n",
    "    ]\n",
    "    df = df.dropna(subset=required_columns)\n",
    "    \n",
    "    # Convert data types\n",
    "    numeric_columns = ['Soil_pH', 'Temperature', 'Humidity', 'Wind_Speed', 'N', 'P', 'K', 'Crop_Yield']\n",
    "    for col_name in numeric_columns:\n",
    "        df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "    categorical_columns = ['Crop_Type', 'Soil_Type']\n",
    "    for col_name in categorical_columns:\n",
    "        df = df.withColumn(col_name, col(col_name).cast(\"string\"))\n",
    "\n",
    "    # Categorical encoding\n",
    "    indexers = [\n",
    "        StringIndexer(inputCol=column, outputCol=f\"{column}_indexed\", handleInvalid=\"keep\")\n",
    "        for column in [\"Crop_Type\", \"Soil_Type\"]\n",
    "    ]\n",
    "\n",
    "    # Define feature columns\n",
    "    feature_columns = [\n",
    "        'Soil_pH', 'Temperature', 'Humidity', 'Wind_Speed', \n",
    "        'N', 'P', 'K', 'Crop_Type_indexed', 'Soil_Type_indexed'\n",
    "    ]\n",
    "\n",
    "    # Assemble features\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_columns, \n",
    "        outputCol=\"features\", \n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "    # Random Forest Regressor\n",
    "    rf = RandomForestRegressor(\n",
    "        featuresCol=\"scaled_features\", \n",
    "        labelCol=\"Crop_Yield\", \n",
    "        maxDepth=10, \n",
    "        numTrees=50\n",
    "    )\n",
    "\n",
    "    # Build pipeline\n",
    "    pipeline = Pipeline(stages=indexers + [assembler, scaler, rf])\n",
    "\n",
    "    # Split data\n",
    "    train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Ensure training data is not empty\n",
    "    if train_data.count() == 0:\n",
    "        raise ValueError(\"Training data is empty. Check the input DataFrame or split ratio.\")\n",
    "\n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    return {\"model\": model, \"test_data\": test_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Column Names: ['Date', 'Crop_Type', 'Soil_Type', 'Soil_pH', 'Temperature', 'Humidity', 'Wind_Speed', 'N', 'P', 'K', 'Crop_Yield', 'Soil_Quality']\n",
      "Cleaned Column Names: ['Date', 'Crop_Type', 'Soil_Type', 'Soil_pH', 'Temperature', 'Humidity', 'Wind_Speed', 'N', 'P', 'K', 'Crop_Yield', 'Soil_Quality']\n"
     ]
    }
   ],
   "source": [
    "df = load_data(\"C:/Users/rabhi/Downloads/big_data/crop-yield-dataset.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-------+------------------+-----------------+------------------+-----------------+----+----+------------------+------------------+\n",
      "|      Date|Crop_Type|Soil_Type|Soil_pH|       Temperature|         Humidity|        Wind_Speed|                N|   P|   K|        Crop_Yield|      Soil_Quality|\n",
      "+----------+---------+---------+-------+------------------+-----------------+------------------+-----------------+----+----+------------------+------------------+\n",
      "|01/01/2014|    Wheat|    Peaty|    5.5| 9.440599409765397|             80.0| 10.95670655406815|60.50000000000001|45.0|31.5|               0.0|22.833333333333332|\n",
      "|01/01/2014|     Corn|    Loamy|    6.5|20.052576424032633|79.94742357596736| 8.591576842195144|             84.0|66.0|50.0|104.87131032861858| 66.66666666666667|\n",
      "|01/01/2014|     Rice|    Peaty|    5.5|12.143099171229291|             80.0| 7.227751486758685|             71.5|54.0|38.5|               0.0|27.333333333333332|\n",
      "|01/01/2014|   Barley|    Sandy|   6.75|19.751848411984515|             80.0|2.6826825548319304|             50.0|40.0|30.0| 58.93979633901676|              35.0|\n",
      "|01/01/2014|  Soybean|    Peaty|    5.5|16.110394507823717|             80.0| 7.696070494170769|             49.5|45.0|38.5| 32.97041272118097|22.166666666666668|\n",
      "+----------+---------+---------+-------+------------------+-----------------+------------------+-----------------+----+----+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = preprocess_data2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "feature_correlation(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_model_data2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare Data for Training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data, test_data, pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_model_data2\u001b[49m(preprocessed_df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prepare_model_data2' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare Data for Training\n",
    "train_data, test_data, pipeline = prepare_model_data2(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Training\n",
    "result = prepare_and_train_model(preprocessed_df)\n",
    "\n",
    "# Extract the training data, test data, and pipeline from the result\n",
    "train_data = result[\"model\"].stages[0].transform(preprocessed_df)  # Get the trained model's stage\n",
    "test_data = result[\"test_data\"]\n",
    "pipeline = result[\"model\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, test_data = prepare_and_train_model(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prepare_and_train_model(preprocessed_df)\n",
    "model = result[\"model\"]\n",
    "test_data = result[\"test_data\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.9526619630853745\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator for R2\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"Crop_Yield\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "# Calculate R2\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "# Print R2\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving model: An error occurred while calling o4404.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n",
      "\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\n",
      "\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n",
      "\t... 68 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Define the model path (use raw string or forward slashes)\n",
    "model_path =\"C:/big_data/Demeter_Prediction_Model\"  # Raw string for Windows paths\n",
    "\n",
    "# Check if directory exists, if not create it\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# Save the model with overwrite option\n",
    "try:\n",
    "    model.write().overwrite().save(model_path)\n",
    "    print(f\"Model successfully saved to {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving model: An error occurred while calling o4529.save.\n",
      ": java.io.IOException: Path C:/big_data/Demeter_Prediction_Model already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Définir le chemin du modèle (utilisez des chaînes brutes ou des slashs pour les chemins Windows)\n",
    "model_path = \"C:/big_data/Demeter_Prediction_Model\"\n",
    "\n",
    "# Vérifier si le répertoire existe, sinon le créer\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "try:\n",
    "    model.save(model_path)  # Sauvegarde du modèle\n",
    "    print(f\"Model successfully saved to {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\rabhi\\Downloads\\big_data\\Demeter_Prediction_Model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if directory exists, if not create it\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"C:/big_data/Demeter_Prediction_Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving model: An error occurred while calling o2842.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n",
      "\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\n",
      "\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n",
      "\t... 68 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.write().overwrite().save(model_path)\n",
    "    print(f\"Model successfully saved to {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
